{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nick088Official/Gemma-Google-Colab/blob/main/MediumT3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Google Gemma AI Models\n",
        "Made by [Nick088](https://linktr.ee/Nick088)"
      ],
      "metadata": {
        "id": "rhAucpX5Li7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install & Load Dependencies, Model\n",
        "!pip install einops\n",
        "!pip install accelerate\n",
        "!pip install huggingface_hub\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "from IPython.display import clear_output\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "Google_Gemma_Model = \"gemma-7b\" #@param ['gemma-2b', 'gemma-2b-it', 'gemma-7b', 'gemma-7b-it']\n",
        "\n",
        "#@markdown The normal version are the official ones from Google which can also run on CPU but takes much RAM, the other ones are GGUF quantized (GPU REQUIRED FOR THOSE), compressed to consume less ram.\n",
        "\n",
        "GGUF_Format = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Go to https://huggingface.co/settings/tokens and make a token with the read role, and paste it here.\n",
        "Hugging_Face_Read_Token = \"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\" #@param {type:\"string\"}\n",
        "\n",
        "# Authenticate\n",
        "login(token=Hugging_Face_Read_Token)\n",
        "\n",
        "if GGUF_Format == False:\n",
        "  from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "  if device == \"cuda\":\n",
        "    model = AutoModelForCausalLM.from_pretrained(f\"google/{Google_Gemma_Model}\", torch_dtype=\"auto\", trust_remote_code=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(f\"google/{Google_Gemma_Model}\", trust_remote_code=True)\n",
        "  else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(f\"google/{Google_Gemma_Model}\", torch_dtype=torch.float32, device_map=\"cpu\", trust_remote_code=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(f\"google/{Google_Gemma_Model}\", trust_remote_code=True)\n",
        "else:\n",
        "  !nvidia-smi\n",
        "  !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
        "  from huggingface_hub import hf_hub_download\n",
        "  from llama_cpp import Llama\n",
        "  model_name = f\"google/{Google_Gemma_Model}\"\n",
        "  model_file = f\"{Google_Gemma_Model}.gguf\"\n",
        "  model_path = hf_hub_download(model_name,\n",
        "                             filename=model_file,\n",
        "                             local_dir='/content',\n",
        "                             token=Hugging_Face_Read_Token)\n",
        "  from llama_cpp import Llama\n",
        "  llm = Llama(model_path=model_path,\n",
        "            n_gpu_layers=-1)\n",
        "\n",
        "clear_output()\n",
        "print(f\"Downloaded {Google_Gemma_Model}\")"
      ],
      "metadata": {
        "id": "GyK68jfLe5gy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Inference\n",
        "#@markdown Your Prompt\n",
        "user_prompt = \"What's Google?\" #@param {type:\"string\"}\n",
        "\n",
        "# Additional inputs\n",
        "#@markdown Add your system prompt (prompt to personalize the AI) here or leave it empty if you wanna use the AI normally\n",
        "system_prompt = \"You are ShortAI, write short but concise responses\"  #@param {type:\"string\"}\n",
        "#@markdown The maximum number of tokens that the model will generate in response to your input\n",
        "max_new_tokens = 1856 #@param {type:\"slider\", min:256, max:2048, step:1}\n",
        "#@markdown Penalize repeated tokens, so make the AI repeat less of itself\n",
        "repetition_penalty = 1.2 #@param {type:\"slider\", min:0.0, max:2, step:0.1}\n",
        "\n",
        "# Concatenate system and user prompts\n",
        "full_prompt = system_prompt + user_prompt\n",
        "\n",
        "if GGUF_Format == False:\n",
        "  # Tokenize the combined prompt\n",
        "  inputs = tokenizer(full_prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "  # Generate text based on the combined prompt and additional inputs\n",
        "  outputs = model.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=max_new_tokens,\n",
        "      repetition_penalty=repetition_penalty\n",
        "      )\n",
        "\n",
        "  text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "  # Extract the generated text from the model output\n",
        "  text = text[len(system_prompt):].strip()  # Remove the system prompt from the generated text\n",
        "  clear_output()\n",
        "  print(text)\n",
        "else:\n",
        "  response = llm(full_prompt, max_tokens=max_new_tokens, repeat_penalty=repetition_penalty)\n",
        "  clear_output()\n",
        "  print(response['choices'][0]['text'])"
      ],
      "metadata": {
        "id": "4901EHafGkhK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}